#!/usr/bin/env python
import os, sys, requests, time
from bs4 import BeautifulSoup as bs

URL = 'https://arxiv.org/list/astro-ph/new'
FLDR = '%s/tmp/arxiv' % os.getenv('HOME')
PDF_URL = 'https://arxiv.org%s'

if __name__ == '__main__':
    if '-h' in sys.argv or '--help' in sys.argv:
        print('''Usage: arxiv_get [-h] [-o] [-c]
    -h: print this message
    -o: save files for offline reading
    -c: remove offline files''')
        quit()
    elif '-c' in sys.argv:
        print('Cleaning up, CTRL+C in 2s to cancel')
        time.sleep(2)
        shutil.rmtree(FLDR + '/pdf')

    os.makedirs(FLDR + '/pdf', exist_ok=True)

    html_req = requests.get(URL)
    if html_req.status_code != 200:
        print(html_req.status_code)
        exit(1)
    html_contents = html_req.text

    arxivsoup = bs(html_contents, 'html.parser')

    # NB: generate updated HTML, then download pdfs in background
    contents_div = arxivsoup.find('div', {'id': 'dlpage'})
    datestr = ' '.join(contents_div.find_next('h3').text.split()[-3: ])
    parseddate = time.strptime(datestr, '%d %B %Y')
    newdate = time.strftime('%y%m%d', parseddate)
    filename = '%s/%s.arxiv' % (FLDR, newdate)

    os.makedirs((FLDR + '/pdf/%s.arxiv') % newdate, exist_ok=True)

    with open(filename, 'w') as f:
        dts = contents_div.find_all('dt')
        dds = contents_div.find_all('dd')
        for dt, dd in zip(dts, dds):
            pdf_link = PDF_URL % dt.find_all('a')[2]['href']
            title = dd.find('div', { 'class': 'list-title' }).contents[1].strip()
            author_bodies = dd.find('div', { 'class': 'list-authors' }).contents[ ::2]
            authors = [e.text for e in author_bodies]
            abstract = dd.find('p').text.strip()
            f.write('%s\n    Authors: %s\n    Link: %s\n    Abstract: %s\n\n' % (
                title,
                ', '.join(authors),
                pdf_link,
                abstract.replace('\n', ''),
            ))
        f.write('%s:' % newdate)
