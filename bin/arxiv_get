#!/usr/bin/env python
from bs4 import BeautifulSoup as bs
import os
import shutil
import requests
import sys
import time
import copy
'''
New version of this script to generate arxiv-like interface but fully offline
- curl astro-ph/new
- download all pdf links
- replace urls with local urls

Typical runtime lesssim 2m
    - respect that arxiv requests no more than 1 connection at a time
'''

URL = 'https://arxiv.org/list/astro-ph/new'
FLDR = '%s/tmp/arxiv' % os.getenv('HOME')
FN = '%s/new.html' % FLDR
PDF_URL = 'https://arxiv.org/%s'

if __name__ == '__main__':
    if '-c' in sys.argv:
        print('Cleaning up, CTRL+C in 2s to cancel')
        time.sleep(2)
        shutil.rmtree(FLDR + '/pdf')

    os.makedirs(FLDR + '/pdf', exist_ok=True)

    html_contents = requests.get(URL).text

    arxivsoup = bs(html_contents, 'html.parser')
    dateline = arxivsoup.find('div', {'class': 'list-dateline'}).text
    announcedate = dateline.split('announced ')[-1]
    parseddate = time.strptime(announcedate, '%a, %d %b %y')
    newdate = time.strftime('%y%m%d', parseddate)

    download_pdfpaths = []

    # NB: generate updated HTML, then download pdfs in background
    contents_div = arxivsoup.find('div', {'id': 'dlpage'})
    for el in contents_div.find_all('a'):
        el_href = el.get('href')
        if el_href is not None:
            if 'pdf' in el_href:
                # convert: '/pdf/123.456' -> 'pdf/123.456.pdf'
                pdf_path = el_href[1: ] + '.pdf'

                # some filenames are '/pdf/astro-ph/123.456.pdf'
                pdf_filename = 'pdf/%s' % pdf_path.split('/')[-1]
                download_pdfpaths.append(pdf_path)
                el['href'] = pdf_filename
                el['target'] = '_blank'

                el2 = copy.copy(el)
                el2.string = 'arXiv'
                el2['href'] = PDF_URL % pdf_path
                el.parent.contents.append(el2)
            else:
                del el['href']

    # save both to new.html & <date>.html, not sure which is more useful
    mysoup = bs('''
<html><head><style>
body {
  background-color: rgb(10, 10, 10);
  color: rgb(220, 220, 220);
}

a:link {
  color: rgb(180, 180, 255);
}
a:visited {
  color: rgb(220, 150, 255);
}
</style></head>
<body></body>
</html>
''', 'html.parser')
    mysoup.body.insert(0, contents_div)
    with open('%s/%s.html' % (FLDR, newdate), 'w') as f1:
        with open('%s/new.html' % FLDR, 'w') as f2:
            for f in [f1, f2]:
                f.write(mysoup.prettify())
    print('Downloaded html')

    if '-o' in sys.argv:
        print('Downloading pdfs for offline reading')
        for pdf_path in download_pdfpaths:
            # some filenames are 'pdf/astro-ph/123.456.pdf'
            pdf_filename = 'pdf/%s' % pdf_path.split('/')[-1]
            pdf_dlpath = '%s/%s' % (FLDR, pdf_filename)
            if os.path.exists(pdf_dlpath):
                print('Exists:', pdf_path)
            else:
                print('Downloading', pdf_path)
                pdf_out = requests.get(PDF_URL % pdf_path)
                with open(pdf_dlpath, 'wb') as f:
                    f.write(pdf_out.content)
    else:
        print('Skipping Downloads')
